{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Generative AI Project Template <p>Template for a new AI Cloud project.</p> <p>Click on Use this template to start your own project!</p> <p></p> <p>This project is a generative ai template. It contains the following features: LLMs, information extraction, chat, rag &amp; evaluation. It uses LLMs(local or cloud),streamlit (with and without fastapi) &amp; Promptfoo as an evaluation and redteam framework for your AI system.</p> Test embeddings Test chat <p>Engineering tools:</p> <ul> <li>[x] Use UV to manage packages</li> <li>[x] pre-commit hooks: use <code>ruff</code> to ensure the code quality &amp; <code>detect-secrets</code> to scan the secrets in the code.</li> <li>[x] Logging using loguru (with colors)</li> <li>[x] Pytest for unit tests</li> <li>[x] Dockerized project (Dockerfile &amp; docker-compose).</li> <li>[x] Streamlit (frontend) &amp; FastAPI (backend)</li> <li>[x] Make commands to handle everything for you: install, run, test</li> </ul> <p>AI tools:</p> <ul> <li>[x] LLM running locally with Ollama or  in the cloud with any LLM provider (LiteLLM)</li> <li>[x] Information extraction and Question answering from documents</li> <li>[x] Chat to test the AI system</li> <li>[x] Efficient async code using asyncio.</li> <li>[x] AI Evaluation framework: using Promptfoo, Ragas &amp; more...</li> </ul> <p>CI/CD &amp; Maintenance tools:</p> <ul> <li>[x] CI/CD pipelines: <code>.github/workflows</code> for GitHub (Testing the AI system, local models with Ollama and the dockerized app)</li> <li>[x] Local CI/CD pipelines: GitHub Actions using <code>github act</code></li> <li>[x] GitHub Actions for deploying to GitHub Pages with mkdocs gh-deploy</li> <li>[x] Dependabot <code>.github/dependabot.yml</code> for automatic dependency and security updates</li> </ul> <p>Documentation tools:</p> <ul> <li>[x] Wiki creation and setup of documentation website using Mkdocs</li> <li>[x] GitHub Pages deployment using mkdocs gh-deploy plugin</li> </ul> <p>Upcoming features: - [ ] add RAG again - [ ] optimize caching in CI/CD -  (https://docs.github.com/en/communities/using-templates-to-encourage-useful-issues-and-pull-requests/creating-a-pull-request-template-for-your-repository) - [ ] Additional MLOps templates: https://github.com/fmind/mlops-python-package - [ ] Add MLFlow - [ ] add Langfuse</p>"},{"location":"#1-getting-started","title":"1. Getting started","text":"<p>This project contains two parts:</p> <ul> <li>The AI app: contains an AI system (local or cloud), a frontend (streamlit), with an optional backend(fastapi).</li> <li>(optional)The Evaluation Tool: The evaluation tool is used to evaluate the performance and safety of the AI system. It uses promptfoo &amp; RAGAS, Python 3.11 and NVM are needed, but no need to install them by yourself since the project will handle that for you.</li> </ul> <p>The following files are used in the contribution pipeline:</p> <ul> <li><code>.env.example</code>: example of the .env file.</li> <li><code>.env</code> : contains the environment variables used by the app.</li> <li><code>Makefile</code>: contains the commands to run the app locally.</li> <li><code>Dockerfile</code>: the dockerfile used to build the project inside a container. It uses the Makefile commands to run the app.</li> <li><code>.pre-commit-config.yaml</code>: pre-commit hooks configuration file</li> <li><code>pyproject.toml</code>: contains the pytest, ruff &amp; other configurations.</li> <li><code>src/api/log_config.py</code> and <code>src/main_backend.py</code>: uvicorn (fastapi) logging configuration.</li> <li><code>src/utils.py</code>: logger (using logguru) and settings using pydantic.   the frontend.</li> <li><code>.github/workflows/**.yml</code>: GitHub actions configuration files.</li> <li><code>.gitlab-ci.yml</code>: Gitlab CI configuration files.</li> <li><code>.gitignore</code>: contains the files to ignore in the project.</li> </ul> <p>Tree:</p> <pre><code>\n\u251c\u2500\u2500 .env.example # example of the .env file\n\u251c\u2500\u2500 .env # contains the environment variables\n\u251c\u2500\u2500 Dockerfile # the dockerfile used to build the project inside a container. It uses the Makefile commands to run the app.\n\u251c\u2500\u2500 docker-compose.yml # docker-compose configuration file (used to run the frontend and backend in docker)\n\u251c\u2500\u2500 Makefile # contains the commands to run the app (like running the frontend, tests, installing packages, docker...)\n\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 pyproject.toml # uv, dependencies, pytest, ruff &amp; other configurations for the package\n\u251c\u2500\u2500 uv.lock # uv lock file\n\u251c\u2500\u2500 .pre-commit-config.yaml # pre-commit hooks configuration file\n\u251c\u2500\u2500 .gitignore # contains the files to ignore in the project\n\u251c\u2500\u2500 .github\n\u2502   \u251c\u2500\u2500 dependabot.yml # dependabot configuration file\n\u2502   \u2514\u2500\u2500 workflows # GitHub actions configuration files\n\u2502       \u2514\u2500\u2500 test-deploy.yaml\n\u251c\u2500\u2500 mkdocs.yml # mkdocs configuration file\n\u251c\u2500\u2500 scripts\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 gen_doc_stubs.py # mkdocs : generate documentation stubs\n\u251c\u2500\u2500 src\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 api\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 evaluation\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main_backend.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main_frontend.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 settings.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 utils.py # logger (using logguru) and settings using pydantic.\n\u251c\u2500\u2500 CODE_OF_CONDUCT.md\n\u251c\u2500\u2500 CONTRIBUTING.md\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 tests\n</code></pre>"},{"location":"#11-local-prerequisites","title":"1.1. Local Prerequisites","text":"<ul> <li>Ubuntu 22.04 or MacOS</li> <li>git clone the repository</li> <li>UV &amp; Python 3.11 (will be installed by the Makefile)</li> <li>Create a <code>.env</code> file (take a look at the <code>.env.example</code> file)</li> </ul>"},{"location":"#12-steps-for-installation-users","title":"1.2 \u2699\ufe0f Steps for Installation (Users)","text":""},{"location":"#app-ai-fastapi-streamlit","title":"App (AI, FastAPI, Streamlit)","text":"<p>You can run the app in a docker container or locally.</p>"},{"location":"#docker","title":"Docker:","text":"<ul> <li>The <code>docker-compose.yml</code> file is used to run the app in a docker container. It will install the following services: frontend, backend and ollama. Your can comment out ollama if you don't need it.</li> <li>The <code>docker-compose.yml</code> will use the <code>.env.example.docker</code> file to configure the environment variables. Per default, it uses ollama docker container.</li> <li>Run this command : <code>make docker-compose</code> then go to http://localhost:8501</li> </ul>"},{"location":"#local","title":"Local :","text":"<ol> <li>To install the app, run <code>make install-prod</code>.</li> <li>Choose one of the following options:</li> <li>Local model: we use Ollama and litellm to run local models. The default model is <code>qwen2.5:0.5b</code> which is a very lightweight model but can be changed.<ul> <li>Create a <code>.env</code> file (You can copy and paste the <code>.env.example</code> file with <code>cp .env.example .env</code>)</li> <li>Install Ollama (for openai) <code>make install-ollama</code></li> <li>Download the model, run <code>make download-ollama-model</code>. It will download the model present in the <code>OLLAMA_MODEL_NAME</code> var in the <code>.env</code> file (default is <code>qwen2.5:0.5b</code>).</li> <li>Run ollama to emulate openai : <code>make run-ollama</code></li> <li>Run <code>make test-ollama</code>. You should see an output with a response.</li> <li>Discuss with the model : <code>make chat-ollama</code></li> </ul> </li> <li> <p>Cloud model:</p> <ul> <li>Create/update the <code>.env</code> file (You can copy and paste the <code>.env.example</code> file with <code>cp .env.example .env</code>)</li> <li>Follow the litellm naming convention.</li> </ul> </li> <li> <p>Run <code>make test-inference-llm</code> to check if your LLM responds.</p> </li> <li>Run the app:</li> <li>To run the app with Streamlit (and without fastapi), run <code>make run-frontend</code></li> <li>To run the app with both Streamlit and FastAPI, run <code>make run-app</code></li> </ol>"},{"location":"#13-steps-for-installation-contributors-and-maintainers","title":"1.3 \u2699\ufe0f Steps for Installation (Contributors and maintainers)","text":"<p>Check the CONTRIBUTING.md file for more information.</p>"},{"location":"#2-contributing","title":"2. Contributing","text":"<p>Check the CONTRIBUTING.md file for more information.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at &lt;&gt;. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4 and 2.0, and was generated by contributing-gen.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to this project","text":"<p>First off, thanks for taking the time to contribute! \u2764\ufe0f</p>"},{"location":"CONTRIBUTING/#1-code-of-conduct","title":"1. Code of Conduct","text":"<p>This project and everyone participating in it is governed by the Code of Conduct. By participating, you are expected to uphold this code. Please report unacceptable behavior.</p>"},{"location":"CONTRIBUTING/#2-team-members","title":"2. Team members:","text":"<ul> <li>Amine Djeghri</li> </ul>"},{"location":"CONTRIBUTING/#3-best-practices","title":"3. Best practices \ud83d\udca1","text":"<ul> <li>Docstring your functions and classes, it is even more important as it is used to generate the documentation with Mkdocs</li> <li>If you use an IDE (like pycharm), define src the \"source\" folder and test the \"test\" folder so your IDE can help you auto import files</li> <li> <p>Use the <code>make</code> commands to run your code, it is easier and faster than writing the full command (and check the Makefile for all available commands \ud83d\ude09)</p> <ul> <li>Run Use the pre-commit hooks to ensure your code is formatted correctly and is of good quality</li> <li>UV is powerful (multi-thread, package graph solving, rust backend, etc.) use it as much as you can.</li> <li>If you have a lot of data, use Polars for faster and more efficient dataframe processing.</li> <li>If you have CPU intensive tasks, use multiprocessing with python's pool map.</li> </ul> </li> <li> <p>Exceptions:</p> <ul> <li>Always log the exceptions and errors (use loguru) and then raise them <code>py     except Exception as e:       logger.error(e)  # Log the original error  with a personalized message or with e (only the message will be logged)       raise e # All the stack trace will be logged</code></li> <li>Sometimes, you don't need to raise the exception (in a loop for example) to not interrupt the execution.</li> <li>Use if else instead of catching and raising the exception when possible (log and raise also)   <code>py       if not os.path.exists(file_path):           logger.error(f\"File not found: {file_path}. The current directory is: {os.getcwd()}\")           raise FileNotFoundError(f\"The file {file_path} does not exist.\")</code></li> </ul> </li> </ul>"},{"location":"CONTRIBUTING/#4-how-to-contribute","title":"4. How to contribute","text":""},{"location":"CONTRIBUTING/#41-file-structure-tree","title":"4.1 File structure (\ud83c\udf33 Tree)","text":"<p>Check the readme file.</p>"},{"location":"CONTRIBUTING/#42-steps-for-installation-contributors-and-maintainers","title":"4.2 Steps for Installation (Contributors and maintainers)","text":"<ul> <li>The first step is to install, read and test the project as a user</li> <li>Then you can either develop in a container or develop locally</li> </ul>"},{"location":"CONTRIBUTING/#a-local-development","title":"a. Local development","text":"<ul> <li>Requires Debian (Ubuntu 22.04) or MacOS.</li> <li>Python will be installed using uv.</li> <li> <p>git clone the repository</p> </li> <li> <p>To install the dev dependencies (pre-commit, pytest, ruff...), run <code>make install-dev</code></p> </li> <li>run <code>make pre-commit install</code> to install pre-commit hooks</li> <li>To install the GitHub actions locally, run <code>make install-act</code></li> <li>To install the gitlab ci locally, run <code>make install-ci</code></li> </ul>"},{"location":"CONTRIBUTING/#b-or-develop-in-a-container","title":"b. or Develop in a container","text":"<ul> <li>If you have a .venv folder locally, you need to delete it, otherwise it will create a conflict since the project is mounted in the container.</li> <li>You can run a docker image containing the project with <code>make docker-prod</code> (or <code>make docker-dev</code> if you want the project to be mounted in the container).</li> <li>A venv is created inside the container and the dependencies are installed.</li> </ul>"},{"location":"CONTRIBUTING/#43-run-the-test-to-see-if-everything-is-working","title":"4.3. Run the test to see if everything is working","text":"<ul> <li>Test the package with :<ul> <li><code>make test</code> will run all the tests (requires .env file)</li> </ul> </li> </ul>"},{"location":"CONTRIBUTING/#44-pushing-your-work","title":"4.4. Pushing your work","text":"<ul> <li> <p>Before you start working on an issue, please comment on (or create) the issue and wait for it to be assigned to you. If someone has already been assigned but didn't have the time to work on it lately, please communicate with them and ask if they're still working on it. This is to avoid multiple people working on the same issue. Once you have been assigned an issue, you can start working on it. When you are ready to submit your changes, open a pull request. For a detailed pull request tutorial, see this guide.</p> </li> <li> <p>Create a branch from the dev branch and respect the naming convention: <code>feature/your-feature-name</code>    or <code>bugfix/your-bug-name</code>.</p> </li> <li>Before commiting your code :</li> <li>Run <code>make test</code> to run the tests</li> <li>Run <code>make pre-commit</code> to check the code style &amp; linting.</li> <li>Run <code>make deploy-doc-local</code> to update the documentation locally and test the website.</li> <li>(optional) Commit Messages: This project uses Gitmoji for commit messages. It helps to      understand the purpose of the commit through emojis. For example, a commit message with a bug fix can be prefixed with      \ud83d\udc1b. There are also Emojis in GitHub</li> <li>Manually, merge dev branch into your branch to solve and avoid any conflicts. Merging strategy: merge : dev \u2192      your_branch</li> <li>After merging, run <code>make test</code> and <code>make pre-commit</code> again to ensure that the tests are still passing.</li> <li>Update the version in <code>pyproject.toml</code> file</li> <li>If your project is a python package, run <code>make build-pacakge</code> to build the package and create the wheel in the <code>dist</code> folder</li> <li>Run CI/CD Locally: Depending on the platform you use:</li> <li>GitHub Actions: run <code>make install-act</code> then <code>make act</code> for GitHub Actions</li> <li>Create a pull request. If the GitHub actions pass, the PR will be accepted and merged to dev.</li> </ul>"},{"location":"CONTRIBUTING/#45-for-repository-maintainers-merging-strategies-github-actions-guidelines","title":"4.5. (For repository maintainers) Merging strategies &amp; GitHub actions guidelines**","text":"<ul> <li>Once the dev branch is tested, the pipeline is green, and the PR has been accepted, you can merge with a 'merge'   strategy.</li> <li>DEV \u2192 MAIN: Then, you should create a merge from dev to main with Squash strategy.</li> <li>MAIN \u2192 RELEASE: The status of the ticket will change then to 'done.'</li> </ul>"},{"location":"package/main_backend/","title":"Main backend","text":""},{"location":"package/main_frontend/","title":"Main frontend","text":""},{"location":"package/settings_env/","title":"Settings env","text":""},{"location":"package/settings_env/#src.settings_env.AzureAISearchEnvironmentVariables","title":"<code>AzureAISearchEnvironmentVariables</code>","text":"<p>               Bases: <code>BaseEnvironmentVariables</code></p> <p>Represents environment variables for configuring Azure AI Search and Azure Storage.</p> Source code in <code>src/settings_env.py</code> <pre><code>class AzureAISearchEnvironmentVariables(BaseEnvironmentVariables):\n    \"\"\"Represents environment variables for configuring Azure AI Search and Azure Storage.\"\"\"\n\n    ################ Azure Search settings ################\n    ENABLE_AZURE_SEARCH: bool = False\n    AZURE_SEARCH_SERVICE_ENDPOINT: Optional[str] = None\n    AZURE_SEARCH_INDEX_NAME: Optional[str] = None\n    AZURE_SEARCH_INDEXER_NAME: Optional[str] = None\n    AZURE_SEARCH_API_KEY: Optional[str] = None\n    AZURE_SEARCH_TOP_K: Optional[str] = \"2\"\n    SEMENTIC_CONFIGURATION_NAME: Optional[str] = None\n    # Azure Storage settings\n    AZURE_STORAGE_ACCOUNT_NAME: Optional[str] = None\n    AZURE_STORAGE_ACCOUNT_KEY: Optional[str] = None\n    AZURE_CONTAINER_NAME: Optional[str] = None\n\n    def get_azure_search_env_vars(self):\n        items_dict = {\n            \"ENABLE_AZURE_SEARCH\": self.ENABLE_AZURE_SEARCH,\n            \"SEMENTIC_CONFIGURATION_NAME\": self.SEMENTIC_CONFIGURATION_NAME,\n            \"AZURE_STORAGE_ACCOUNT_NAME\": self.AZURE_STORAGE_ACCOUNT_NAME,\n            \"AZURE_STORAGE_ACCOUNT_KEY\": self.AZURE_STORAGE_ACCOUNT_KEY,\n            \"AZURE_CONTAINER_NAME\": self.AZURE_CONTAINER_NAME,\n        }\n\n        items_dict.update(\n            {key: value for key, value in vars(self).items() if key.startswith(\"AZURE_SEARCH\")}\n        )\n        return items_dict\n\n    @model_validator(mode=\"after\")\n    def check_ai_search_keys(self: Self) -&gt; Self:\n        \"\"\"Validate API keys based on the selected provider after model initialization.\"\"\"\n        if self.ENABLE_AZURE_SEARCH:\n            azure_search_vars = self.get_azure_search_env_vars()\n            if any(value is None for value in azure_search_vars.values()):\n                loguru_logger.error(\n                    \"\\nAZURE_SEARCH environment variables must be provided when ENABLE_AZURE_SEARCH is True.\"\n                    f\"\\n{pretty_repr(azure_search_vars)}\"\n                )\n                raise ValueError(\n                    \"\\nAZURE_SEARCH environment variables must be provided when ENABLE_AZURE_SEARCH is True.\"\n                    f\"\\n{pretty_repr(azure_search_vars)}\"\n                )\n        return self\n</code></pre>"},{"location":"package/settings_env/#src.settings_env.AzureAISearchEnvironmentVariables.check_ai_search_keys","title":"<code>check_ai_search_keys()</code>","text":"<p>Validate API keys based on the selected provider after model initialization.</p> Source code in <code>src/settings_env.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_ai_search_keys(self: Self) -&gt; Self:\n    \"\"\"Validate API keys based on the selected provider after model initialization.\"\"\"\n    if self.ENABLE_AZURE_SEARCH:\n        azure_search_vars = self.get_azure_search_env_vars()\n        if any(value is None for value in azure_search_vars.values()):\n            loguru_logger.error(\n                \"\\nAZURE_SEARCH environment variables must be provided when ENABLE_AZURE_SEARCH is True.\"\n                f\"\\n{pretty_repr(azure_search_vars)}\"\n            )\n            raise ValueError(\n                \"\\nAZURE_SEARCH environment variables must be provided when ENABLE_AZURE_SEARCH is True.\"\n                f\"\\n{pretty_repr(azure_search_vars)}\"\n            )\n    return self\n</code></pre>"},{"location":"package/settings_env/#src.settings_env.EvaluatorEnvironmentVariables","title":"<code>EvaluatorEnvironmentVariables</code>","text":"<p>               Bases: <code>BaseEnvironmentVariables</code></p> Source code in <code>src/settings_env.py</code> <pre><code>class EvaluatorEnvironmentVariables(BaseEnvironmentVariables):\n    EVALUATOR_BASE_URL: Optional[str] = \"http://localhost:11434\"\n    EVALUATOR_API_KEY: Optional[SecretStr] = \"tt\"\n    EVALUATOR_DEPLOYMENT_NAME: Optional[str] = \"ollama_chat/qwen2.5:0.5b\"\n    EVALUATOR_API_VERSION: str = \"2024-10-01-preview\"\n\n    ENABLE_EVALUATION: bool = False\n\n    def get_evaluator_env_vars(self):\n        return {\n            \"EVALUATOR_BASE_URL\": self.EVALUATOR_BASE_URL,\n            \"EVALUATOR_API_KEY\": self.EVALUATOR_API_KEY,\n            \"EVALUATOR_DEPLOYMENT_NAME\": self.EVALUATOR_DEPLOYMENT_NAME,\n        }\n\n    @model_validator(mode=\"after\")\n    def check_eval_api_keys(self: Self) -&gt; Self:\n        \"\"\"Validate API keys based on the selected provider after model initialization.\"\"\"\n        if self.ENABLE_EVALUATION:\n            eval_vars = self.get_evaluator_env_vars()\n            if any(value is None for value in eval_vars.values()):\n                # loguru_logger.opt(exception=True).error(\"Your error message\")\n                loguru_logger.error(\n                    \"\\nEVALUATION environment variables must be provided when ENABLE_EVALUATION is True.\"\n                    f\"\\n{pretty_repr(eval_vars)}\"\n                )\n                raise ValueError(\n                    \"\\nEVALUATION environment variables must be provided when ENABLE_EVALUATION is True.\"\n                    f\"\\n{pretty_repr(eval_vars)}\"\n                )\n\n        return self\n</code></pre>"},{"location":"package/settings_env/#src.settings_env.EvaluatorEnvironmentVariables.check_eval_api_keys","title":"<code>check_eval_api_keys()</code>","text":"<p>Validate API keys based on the selected provider after model initialization.</p> Source code in <code>src/settings_env.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_eval_api_keys(self: Self) -&gt; Self:\n    \"\"\"Validate API keys based on the selected provider after model initialization.\"\"\"\n    if self.ENABLE_EVALUATION:\n        eval_vars = self.get_evaluator_env_vars()\n        if any(value is None for value in eval_vars.values()):\n            # loguru_logger.opt(exception=True).error(\"Your error message\")\n            loguru_logger.error(\n                \"\\nEVALUATION environment variables must be provided when ENABLE_EVALUATION is True.\"\n                f\"\\n{pretty_repr(eval_vars)}\"\n            )\n            raise ValueError(\n                \"\\nEVALUATION environment variables must be provided when ENABLE_EVALUATION is True.\"\n                f\"\\n{pretty_repr(eval_vars)}\"\n            )\n\n    return self\n</code></pre>"},{"location":"package/settings_env/#src.settings_env.Settings","title":"<code>Settings</code>","text":"<p>               Bases: <code>InferenceEnvironmentVariables</code>, <code>EmbeddingsEnvironmentVariables</code>, <code>EvaluatorEnvironmentVariables</code>, <code>AzureAISearchEnvironmentVariables</code></p> <p>Settings class for the application.</p> <p>This class is automatically initialized with environment variables from the .env file. It inherits from the following classes and contains additional settings for streamlit and fastapi - ChatEnvironmentVariables - AzureAISearchEnvironmentVariables - EvaluationEnvironmentVariables</p> Source code in <code>src/settings_env.py</code> <pre><code>class Settings(\n    InferenceEnvironmentVariables,\n    EmbeddingsEnvironmentVariables,\n    EvaluatorEnvironmentVariables,\n    AzureAISearchEnvironmentVariables,\n):\n    \"\"\"Settings class for the application.\n\n    This class is automatically initialized with environment variables from the .env file.\n    It inherits from the following classes and contains additional settings for streamlit and fastapi\n    - ChatEnvironmentVariables\n    - AzureAISearchEnvironmentVariables\n    - EvaluationEnvironmentVariables\n\n    \"\"\"\n\n    FASTAPI_HOST: str = \"localhost\"\n    FASTAPI_PORT: int = 8080\n    STREAMLIT_PORT: int = 8501\n    DEV_MODE: bool = True\n\n    def get_active_env_vars(self):\n        env_vars = {\n            \"DEV_MODE\": self.DEV_MODE,\n            \"FASTAPI_PORT\": self.FASTAPI_PORT,\n            \"STREAMLIT_PORT\": self.STREAMLIT_PORT,\n        }\n\n        env_vars.update(self.get_inference_env_vars())\n        env_vars.update(self.get_embeddings_env_vars())\n        if self.ENABLE_AZURE_SEARCH:\n            env_vars.update(self.get_azure_search_env_vars())\n\n        if self.ENABLE_EVALUATION:\n            env_vars.update(self.get_evaluator_env_vars())\n\n        return env_vars\n</code></pre>"},{"location":"package/utils/","title":"Utils","text":""},{"location":"package/utils/#src.utils.initialize","title":"<code>initialize()</code>","text":"<p>Initialize the settings, logger, and search client.</p> <p>Reads the environment variables from the .env file defined in the Settings class.</p> <p>Returns:</p> Type Description <p>settings</p> <p>loguru_logger</p> <p>search_client</p> Source code in <code>src/utils.py</code> <pre><code>def initialize():\n    \"\"\"Initialize the settings, logger, and search client.\n\n    Reads the environment variables from the .env file defined in the Settings class.\n\n    Returns:\n        settings\n        loguru_logger\n        search_client\n    \"\"\"\n    settings = Settings()\n    loguru_logger.remove()\n\n    if settings.DEV_MODE:\n        loguru_logger.add(sys.stderr, level=\"TRACE\")\n    else:\n        loguru_logger.add(sys.stderr, level=\"INFO\")\n\n    search_client = None\n    if settings.ENABLE_AZURE_SEARCH:\n        search_client = SearchClient(\n            settings.AZURE_SEARCH_SERVICE_ENDPOINT,\n            settings.AZURE_SEARCH_INDEX_NAME,\n            AzureKeyCredential(settings.AZURE_SEARCH_API_KEY),\n        )\n\n    return settings, loguru_logger, search_client\n</code></pre>"},{"location":"package/api/api/","title":"Api","text":""},{"location":"package/api/api_route/","title":"Api route","text":""},{"location":"package/api/api_route/#src.api.api_route.TagEnum","title":"<code>TagEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>API tags.</p> Source code in <code>src/api/api_route.py</code> <pre><code>class TagEnum(str, Enum):\n    \"\"\"API tags.\"\"\"\n\n    general = \"general\"\n    tag_example = \"tag_example\"\n</code></pre>"},{"location":"package/api/log_config/","title":"Log config","text":""},{"location":"package/evaluation/context/","title":"Context","text":""},{"location":"package/evaluation/configs/config_baseline/","title":"Config baseline","text":""},{"location":"package/evaluation/configs/config_json/","title":"Config json","text":""},{"location":"package/evaluation/configs/config_json/#src.evaluation.configs.config_json.call_api","title":"<code>call_api(prompt, options, context)</code>","text":"<p>Function used by default by promptfoo. Check the config_json.yml.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt used in the configuration file (prompts section of config_json.yml).</p> required <code>options</code> required <code>context</code> <code>dict</code> <p>A dictionary containing the other_vars and context return by the previous function get_var</p> required Source code in <code>src/evaluation/configs/config_json.py</code> <pre><code>def call_api(prompt, options, context) -&gt; dict[str, str]:\n    \"\"\"Function used by default by promptfoo. Check the config_json.yml.\n\n    Args:\n        prompt (str): The prompt used in the configuration file (prompts section of config_json.yml).\n        options:\n        context (dict): A dictionary containing the other_vars and context return by the previous function get_var\n\n\n    \"\"\"\n    query = safe_eval(context[\"vars\"][\"query\"])\n    output = {list(query.keys())[0]: \"test\"}\n    result = {\n        \"output\": json.dumps(output, ensure_ascii=False),\n    }\n\n    return result\n</code></pre>"},{"location":"package/evaluation/configs/config_json/#src.evaluation.configs.config_json.get_var","title":"<code>get_var(var_name, prompt, other_vars)</code>","text":"<p>Function used by default by promptfoo call from the column 'context' of the dataset used in config_json.yml (test_json.csv).</p> <p>This function returns the context that will be used in the following call_api function The context can be. For example, the retrieved list of documents this is an example, and we will return the context that is defined in the csv file other_vars contains the vars of the csv file. prompt contains the prompt in the config_json.yml</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt used in the configuration file (prompts section of config_json.yml).</p> required <code>other_vars</code> <code>dict</code> <p>A dictionary containing variables from a CSV file.</p> required Source code in <code>src/evaluation/configs/config_json.py</code> <pre><code>def get_var(var_name, prompt, other_vars):\n    \"\"\"Function used by default by promptfoo call from the column 'context' of the dataset used in config_json.yml (test_json.csv).\n\n    This function returns the context that will be used in the following call_api function\n    The context can be. For example, the retrieved list of documents\n    this is an example, and we will return the context that is defined in the csv file\n    other_vars contains the vars of the csv file. prompt contains the prompt in the config_json.yml\n\n    Args:\n        prompt (str): The prompt used in the configuration file (prompts section of config_json.yml).\n        other_vars (dict): A dictionary containing variables from a CSV file.\n\n    \"\"\"\n    context = [\n        \"The USA Supreme Court ruling on abortion has sparked intense debates and discussions not only within the country but also around the world.\",\n        \"Many countries look to the United States as a leader in legal and social issues, so the decision could potentially influence the policies and attitudes towards abortion in other nations.\",\n        \"The ruling may impact international organizations and non-governmental groups that work on reproductive rights and women's health issues.\",\n    ]\n    return {\"output\": json.dumps(context, ensure_ascii=False)}\n</code></pre>"},{"location":"package/evaluation/metrics/data_types/","title":"Data types","text":""},{"location":"package/evaluation/metrics/utils/","title":"Utils","text":""},{"location":"package/evaluation/metrics/information_extraction/entity_level/","title":"Entity level","text":""},{"location":"package/evaluation/metrics/information_extraction/exact_match_json/","title":"Exact match json","text":""},{"location":"package/evaluation/metrics/information_extraction/exact_match_json/#src.evaluation.metrics.information_extraction.exact_match_json.get_assert","title":"<code>get_assert(output, context)</code>","text":"<p>Evaluates the precision at k.</p> Source code in <code>src/evaluation/metrics/information_extraction/exact_match_json.py</code> <pre><code>def get_assert(output: str, context):\n    \"\"\"Evaluates the precision at k.\"\"\"\n    threshold = 0.99\n    llm_answer, true_answer = convert_to_json(output, context, threshold)\n\n    try:\n        model_true_answer = create_dynamic_model(true_answer)\n        true_answer = model_true_answer(**true_answer)\n\n        llm_answer = model_true_answer(**llm_answer)\n\n        if llm_answer == true_answer:\n            score = 1.0\n            reason = f\"{score} &gt; {threshold} = {score &gt; threshold}\"\n        else:\n            dict_a = llm_answer.model_dump()\n            dict_b = true_answer.model_dump()\n            differences = [key for key in dict_b.keys() if dict_a.get(key) != dict_b.get(key)]\n\n            score = round(float(1 - (len(differences) / len(llm_answer.model_fields))), 2)\n\n            reason = f\"{score} &gt; {threshold} = {score &gt; threshold}. Number of differences: {len(differences)}. Differences: {differences}\"\n\n    except ValidationError as e:\n        total_fields = len(llm_answer.model_fields)\n        errors_count = len(e.errors())\n        score = round(float(1 - (errors_count / total_fields)), 2)\n        reason = str(e)\n\n    return {\n        \"pass\": score &gt; threshold,\n        \"score\": score,\n        \"reason\": reason,\n    }\n</code></pre>"},{"location":"package/evaluation/metrics/information_extraction/missing_fields/","title":"Missing fields","text":""},{"location":"package/evaluation/metrics/information_extraction/missing_fields/#src.evaluation.metrics.information_extraction.missing_fields.get_assert","title":"<code>get_assert(output, context)</code>","text":"<p>Evaluates the precision at k.</p> Source code in <code>src/evaluation/metrics/information_extraction/missing_fields.py</code> <pre><code>def get_assert(output: str, context) -&gt; GradingResult:\n    \"\"\"Evaluates the precision at k.\"\"\"\n    threshold = 0.99\n\n    llm_answer, true_answer = convert_to_json(output, context, threshold)\n\n    try:\n        model_true_answer = create_dynamic_model(true_answer)\n        # true_answer = model_true_answer(**true_answer)\n\n        llm_answer = model_true_answer(**llm_answer)\n        null_fields = [key for key, value in llm_answer.model_dump().items() if value is None]\n\n        score = round(float(1 - (len(null_fields) / len(llm_answer.model_fields))), 2)\n\n        reason = (\n            f\"{score} &gt; {threshold} = {score &gt; threshold}. Number of null fields: {len(null_fields)}. \"\n            f\"null_fields: {null_fields}\"\n        )\n    except ValidationError as e:\n        error = validation_error_message(e)\n        total_fields = len(llm_answer.model_fields)\n        errors_count = len(error.errors())\n        score = float(1 - (errors_count / total_fields))\n        reason = str(error)\n\n    return {\n        \"pass\": score &gt; threshold,\n        \"score\": score,\n        \"reason\": reason,\n    }\n</code></pre>"},{"location":"package/evaluation/metrics/information_extraction/similarity_json/","title":"Similarity json","text":""},{"location":"package/evaluation/metrics/information_extraction/similarity_json/#src.evaluation.metrics.information_extraction.similarity_json.compare_pydantic_objects","title":"<code>compare_pydantic_objects(obj1, obj2, differences=None)</code>","text":"<p>Compare two Pydantic objects using cosine similarity.</p> Source code in <code>src/evaluation/metrics/information_extraction/similarity_json.py</code> <pre><code>def compare_pydantic_objects(\n    obj1: BaseModel, obj2: BaseModel, differences: list = None\n) -&gt; dict[str, float]:\n    \"\"\"Compare two Pydantic objects using cosine similarity.\"\"\"\n    result = {}\n    total_similarity = 0\n    similarity = 0\n    if not differences:\n        differences = obj1.model_fields\n\n    for field in differences:\n        value1 = getattr(obj1, field)\n        value2 = getattr(obj2, field)\n        if value1 != value2:\n            if value1 and value2:\n                embedding1 = llmaaj_embedding_client.embed_query(text=str(value1))\n                embedding2 = llmaaj_embedding_client.embed_query(text=str(value2))\n                similarity = round(cosine_similarity(embedding1, embedding2), 2)\n            else:\n                similarity = 0\n        else:\n            similarity = 1\n\n        result[field] = similarity\n        total_similarity += similarity\n    return result, total_similarity\n</code></pre>"},{"location":"package/evaluation/metrics/information_extraction/similarity_json/#src.evaluation.metrics.information_extraction.similarity_json.cosine_similarity","title":"<code>cosine_similarity(a, b)</code>","text":"<p>Calculate cosine similarity between two vectors.</p> Source code in <code>src/evaluation/metrics/information_extraction/similarity_json.py</code> <pre><code>def cosine_similarity(a: np.ndarray, b: np.ndarray) -&gt; float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n</code></pre>"},{"location":"package/evaluation/metrics/information_extraction/similarity_json/#src.evaluation.metrics.information_extraction.similarity_json.get_assert","title":"<code>get_assert(output, context)</code>","text":"<p>Evaluates the precision at k.</p> Source code in <code>src/evaluation/metrics/information_extraction/similarity_json.py</code> <pre><code>def get_assert(output: str, context):\n    \"\"\"Evaluates the precision at k.\"\"\"\n    threshold = 0.99\n    llm_answer, true_answer = convert_to_json(output, context, threshold)\n\n    try:\n        model_true_answer = create_dynamic_model(true_answer)\n        true_answer = model_true_answer(**true_answer)\n\n        llm_answer = model_true_answer(**llm_answer)\n\n        if llm_answer == true_answer:\n            score = 1.0\n            reason = f\"{score} &gt; {threshold} = {score &gt; threshold}\"\n        else:\n            dict_a = llm_answer.model_dump()\n            dict_b = true_answer.model_dump()\n            differences = [key for key in dict_b.keys() if dict_a.get(key) != dict_b.get(key)]\n\n            num_similar_fields = len(llm_answer.model_fields) - len(differences)\n\n            result, similarity = compare_pydantic_objects(llm_answer, true_answer, differences)\n            score = round(\n                float((num_similar_fields + similarity) / len(llm_answer.model_fields)),\n                2,\n            )\n\n            reason = f\"{score} &gt; {threshold} = {score &gt; threshold}. Number of differences: {len(differences)}. Differences: {result}\"\n\n    except ValidationError as e:\n        total_fields = len(llm_answer.model_fields)\n        errors_count = len(e.errors())\n        score = round(float(1 - (errors_count / total_fields)), 2)\n        reason = str(e)\n\n    return {\n        \"pass\": score &gt; threshold,\n        \"score\": score,\n        \"reason\": reason,\n    }\n</code></pre>"},{"location":"package/evaluation/metrics/order_aware/reciprocal_rank/","title":"Reciprocal rank","text":""},{"location":"package/evaluation/metrics/order_aware/reciprocal_rank/#src.evaluation.metrics.order_aware.reciprocal_rank.get_assert","title":"<code>get_assert(output, context)</code>","text":"<p>Evaluates the precision at k.</p> Source code in <code>src/evaluation/metrics/order_aware/reciprocal_rank.py</code> <pre><code>def get_assert(output: str, context) -&gt; GradingResult:\n    \"\"\"Evaluates the precision at k.\"\"\"\n    retrieved_docs = safe_eval(context[\"vars\"][\"context\"])\n    relevant_docs = safe_eval(context[\"vars\"][\"relevant_context\"])\n\n    score = 0\n    # compute Reciprocal Rank\n    try:\n        score = round(1 / (relevant_docs.index(retrieved_docs[0]) + 1), 2)\n    except ValueError:\n        score = -1\n\n    # threshold = context[\"test\"][\"metadata\"][\"threshold_ragas_as\"]\n    threshold = 0\n\n    if math.isnan(score):\n        score = 0.0\n\n    return {\n        \"pass\": score &gt; threshold,\n        \"score\": score,\n        \"reason\": f\"{score} &gt; {threshold} = {score &gt; threshold}\",\n    }\n</code></pre>"},{"location":"package/evaluation/metrics/order_unaware/f1_at_k/","title":"F1 at k","text":""},{"location":"package/evaluation/metrics/order_unaware/f1_at_k/#src.evaluation.metrics.order_unaware.f1_at_k.get_assert","title":"<code>get_assert(output, context)</code>","text":"<p>Calculates F1@k.</p> Source code in <code>src/evaluation/metrics/order_unaware/f1_at_k.py</code> <pre><code>def get_assert(output: str, context) -&gt; GradingResult:\n    \"\"\"Calculates F1@k.\"\"\"\n    precision = precision_at_k.get_assert(context=context, output=output)[\"score\"]\n    recall = recall_at_k.get_assert(context=context, output=output)[\"score\"]\n\n    if precision + recall == 0:\n        score = 0.0\n    else:\n        score = round(float(2 * (precision * recall) / (precision + recall)), 2)\n\n    # threshold = context[\"test\"][\"metadata\"][\"threshold_ragas_as\"]\n    threshold = 0\n\n    if math.isnan(score):\n        score = 0.0\n\n    return {\n        \"pass\": score &gt; threshold,\n        \"score\": score,\n        \"reason\": f\"{score} &gt; {threshold} = {score &gt; threshold}\",\n    }\n</code></pre>"},{"location":"package/evaluation/metrics/order_unaware/precision_at_k/","title":"Precision at k","text":""},{"location":"package/evaluation/metrics/order_unaware/precision_at_k/#src.evaluation.metrics.order_unaware.precision_at_k.get_assert","title":"<code>get_assert(output, context)</code>","text":"<p>Evaluates the precision at k.</p> Source code in <code>src/evaluation/metrics/order_unaware/precision_at_k.py</code> <pre><code>def get_assert(output: str, context) -&gt; GradingResult:\n    \"\"\"Evaluates the precision at k.\"\"\"\n    retrieved_docs = safe_eval(context[\"vars\"][\"context\"])\n    relevant_docs = safe_eval(context[\"vars\"][\"relevant_context\"])\n    k = os.environ.get(\"K\", 3)\n    retrieved_docs_at_k = retrieved_docs[:k]\n    relevant_count = sum([1 for doc in retrieved_docs_at_k if doc in relevant_docs])\n    score = float(relevant_count / k)\n\n    # threshold = context[\"test\"][\"metadata\"][\"threshold_ragas_as\"]\n    threshold = 0\n\n    if math.isnan(score):\n        score = 0.0\n\n    return {\n        \"pass\": score &gt; threshold,\n        \"score\": score,\n        \"reason\": f\"{score} &gt; {threshold} = {score &gt; threshold}\",\n    }\n</code></pre>"},{"location":"package/evaluation/metrics/order_unaware/recall_at_k/","title":"Recall at k","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_answer_correctness/","title":"Ragas answer correctness","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_answer_relevancy/","title":"Ragas answer relevancy","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_answer_similarity/","title":"Ragas answer similarity","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_context_entity_recall/","title":"Ragas context entity recall","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_context_precision/","title":"Ragas context precision","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_context_recall/","title":"Ragas context recall","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_context_utilization/","title":"Ragas context utilization","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_faithfulness/","title":"Ragas faithfulness","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_harmfulness/","title":"Ragas harmfulness","text":""},{"location":"package/ml/ai/","title":"Ai","text":""},{"location":"package/ml/ai/#src.ml.ai.get_completions","title":"<code>get_completions(messages, stream=False, response_model=None, max_tokens=1000, temperature=0, top_p=1, seed=100, full_response=False, client=None)</code>","text":"<p>Returns a response from the azure openai model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> required <code>stream</code> <code>bool</code> <code>False</code> <code>response_model</code> <code>BaseModel</code> <code>None</code> <code>monitor</code> required <code>max_tokens</code> <code>int</code> <code>1000</code> <code>temperature</code> <code>int</code> <code>0</code> <code>top_p</code> <code>int</code> <code>1</code> <code>seed</code> <code>int</code> <code>100</code> <code>full_response</code> <code>bool</code> <code>False</code> <code>client</code> <code>None</code> <p>Returns:</p> Name Type Description <code>response</code> <code>str | BaseModel | None</code> <p>str | BaseModel | None :</p> Source code in <code>src/ml/ai.py</code> <pre><code>def get_completions(\n    messages: list,\n    stream: bool = False,\n    response_model: BaseModel = None,  # Use Instructor library\n    max_tokens: int = 1000,\n    temperature: int = 0,\n    top_p: int = 1,\n    seed: int = 100,\n    full_response: bool = False,\n    client=None,\n) -&gt; str | BaseModel | None:\n    \"\"\"Returns a response from the azure openai model.\n\n    Args:\n        messages:\n        stream:\n        response_model:\n        monitor:\n        max_tokens:\n        temperature:\n        top_p:\n        seed:\n        full_response:\n        client:\n\n    Returns:\n        response : str | BaseModel | None :\n    \"\"\"\n    input_dict = {\n        \"model\": \"aasasa\",\n        \"messages\": messages,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"seed\": seed,\n        \"stream\": stream,\n    }\n    # if response_model:\n    #     # if you use local models instead of openai models, the response_model feature may not work\n    #     client = instructor.from_openai(chat_client, mode=instructor.Mode.JSON)\n    #     input_dict[\"response_model\"] = response_model\n\n    if stream:\n        raise NotImplementedError(\"Stream is not supported right now. Please set stream to False.\")\n</code></pre>"},{"location":"package/ml/ai/#src.ml.ai.get_rag_response","title":"<code>get_rag_response(user_input)</code>","text":"<p>Return the response after running RAG.</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> required <code>settings</code> required <code>conversation_id</code> required <p>Returns:</p> Name Type Description <code>response</code> Source code in <code>src/ml/ai.py</code> <pre><code>def get_rag_response(user_input):\n    \"\"\"Return the response after running RAG.\n\n    Args:\n        user_input:\n        settings:\n        conversation_id:\n\n    Returns:\n        response:\n\n    \"\"\"\n    logger.info(f\"Running RAG\")\n\n    context = get_related_document_ai_search(user_input)\n    formatted_user_input = f\"question :{user_input}, \\n\\n contexte : \\n{context}.\"\n    logger.info(f\"RAG - final formatted prompt: {formatted_user_input}\")\n\n    response = get_completions(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Tu est un chatbot qui r\u00e9pond aux questions.\",\n            },\n            {\"role\": \"user\", \"content\": formatted_user_input},\n        ],\n    )\n    return response\n</code></pre>"},{"location":"package/ml/ai/#src.ml.ai.run_azure_ai_search_indexer","title":"<code>run_azure_ai_search_indexer()</code>","text":"<p>Run the azure ai search index.</p> <p>Returns:</p> Name Type Description <code>res</code> <p>response</p> Source code in <code>src/ml/ai.py</code> <pre><code>def run_azure_ai_search_indexer():\n    \"\"\"Run the azure ai search index.\n\n    Returns:\n            res: response\n    \"\"\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"api-key\": settings.AZURE_SEARCH_API_KEY,\n    }\n    params = {\"api-version\": \"2024-07-01\"}\n    url = f\"{settings.AZURE_SEARCH_SERVICE_ENDPOINT}/indexers('{settings.AZURE_SEARCH_INDEXER_NAME}')/run\"\n\n    res = requests.post(url=url, headers=headers, params=params)\n    logger.debug(f\"run_azure_ai_search_index response: {res.status_code}\")\n    return res\n</code></pre>"},{"location":"package/ml/llm/","title":"Llm","text":""},{"location":"package/ml/llm/#src.ml.llm.EmbeddingLLMConfig","title":"<code>EmbeddingLLMConfig</code>","text":"<p>               Bases: <code>InferenceLLMConfig</code></p> <p>Configuration for the embedding model.</p> Source code in <code>src/ml/llm.py</code> <pre><code>class EmbeddingLLMConfig(InferenceLLMConfig):\n    \"\"\"Configuration for the embedding model.\"\"\"\n\n    model_name: str\n    base_url: str\n    api_key: SecretStr\n    api_version: str = \"2024-12-01-preview\"  # used only if model is from azure openai\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def load_model(self, prompt: str, schema: Type[BaseModel] = None, *args, **kwargs):\n        pass\n\n    def embed_text(self, text: str) -&gt; list[float]:\n        response = embedding(\n            model=self.model_name,\n            api_base=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            input=[text],\n        )\n        return response.data[0][\"embedding\"]\n\n    def embed_texts(self, texts: list[str]) -&gt; list[list[float]]:\n        response = embedding(\n            model=self.model_name,\n            api_base=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            input=texts,\n        )\n        return [data.embedding for data in response.data]\n\n    async def a_embed_text(self, text: str) -&gt; list[float]:\n        response = await aembedding(\n            model=self.model_name,\n            api_base=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            input=[text],\n        )\n        return response.data[0][\"embedding\"]\n\n    async def a_embed_texts(self, texts: list[str]) -&gt; list[list[float]]:\n        response = await aembedding(\n            model=self.model_name,\n            api_base=self.base_url,\n            api_key=self.api_key.get_secret_value(),\n            input=texts,\n        )\n        return [data.embedding for data in response.data]\n\n    def get_model_name(self):\n        return self.model_name\n</code></pre>"},{"location":"package/ml/llm/#src.ml.llm.InferenceLLMConfig","title":"<code>InferenceLLMConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the inference model.</p> Source code in <code>src/ml/llm.py</code> <pre><code>class InferenceLLMConfig(BaseModel):\n    \"\"\"Configuration for the inference model.\"\"\"\n\n    model_name: str\n    base_url: str\n    api_key: SecretStr\n    api_version: str = \"2024-12-01-preview\"  # used only if model is from azure openai\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    supports_response_schema: bool = False\n\n    temperature: Optional[float] = None\n    seed: int = 1729\n    max_tokens: Optional[int] = None\n\n    @model_validator(mode=\"after\")\n    def init_client(self) -&gt; Self:\n        try:\n            # check if the model supports structured output\n            self.supports_response_schema = supports_response_schema(self.model_name.split(\"/\")[-1])\n            logger.debug(\n                f\"\\nModel: {self.model_name} Supports response schema: {self.supports_response_schema}\"\n            )\n        except Exception as e:\n            # logger.exception(f\"Error in initializing the LLM : {self}\")\n            logger.error(f\"Error in initializing the LLM : {e}\")\n            raise e\n\n        return self\n\n    def load_model(self, prompt: str, schema: Type[BaseModel] = None, *args, **kwargs):\n        pass\n\n    @observe(as_type=\"generation\")\n    async def a_generate(self, prompt: str, schema: Type[BaseModel] = None, *args, **kwargs):\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        return await self.a_generate_from_messages(\n            messages=messages, schema=schema, *args, **kwargs\n        )\n\n    @observe(as_type=\"generation\")\n    @retry(\n        wait=wait_fixed(60),\n        stop=stop_after_attempt(6),\n        retry=retry_if_exception_type(\n            (litellm.exceptions.RateLimitError, instructor.exceptions.InstructorRetryException)\n        ),\n    )\n    async def a_generate_from_messages(\n        self, messages: list, schema: Type[BaseModel] = None, *args, **kwargs\n    ):\n        # check if model supports structured output\n        if schema:\n            if self.supports_response_schema:\n                res = await litellm.acompletion(\n                    model=self.model_name,\n                    api_key=self.api_key.get_secret_value(),\n                    base_url=self.base_url,\n                    messages=messages,\n                    response_format=schema,\n                    api_version=self.api_version,\n                )\n                if res.choices[0].finish_reason == \"content_filter\":\n                    raise ValueError(f\"Response filtred by content filter\")\n                else:\n                    dict_res = ast.literal_eval(res.choices[0].message.content)\n                    return schema(**dict_res)\n\n            else:\n                client = instructor.from_litellm(acompletion, mode=instructor.Mode.JSON)\n                res, raw_completion = await client.chat.completions.create_with_completion(\n                    model=self.model_name,\n                    api_key=self.api_key.get_secret_value(),\n                    base_url=self.base_url,\n                    messages=messages,\n                    response_model=schema,\n                    api_version=self.api_version,\n                )\n                return res\n        else:\n            res = await litellm.acompletion(\n                model=self.model_name,\n                api_key=self.api_key.get_secret_value(),\n                base_url=self.base_url,\n                messages=messages,\n                api_version=self.api_version,\n            )\n            return res.choices[0].message.content\n\n    @observe(as_type=\"generation\")\n    def generate(self, prompt: str, schema: Type[BaseModel] = None, *args, **kwargs):\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        return self.generate_from_messages(messages=messages, schema=schema, *args, **kwargs)\n\n    @observe(as_type=\"generation\")\n    @retry(\n        wait=wait_fixed(60),\n        stop=stop_after_attempt(6),\n        retry=retry_if_exception_type(\n            (litellm.exceptions.RateLimitError, instructor.exceptions.InstructorRetryException)\n        ),\n    )\n    def generate_from_messages(\n        self, messages: list, schema: Type[BaseModel] = None, *args, **kwargs\n    ):\n        try:\n            # check if model supports structured output\n            if schema:\n                if self.supports_response_schema:\n                    res = litellm.completion(\n                        model=self.model_name,\n                        api_key=self.api_key.get_secret_value(),\n                        base_url=self.base_url,\n                        messages=messages,\n                        response_format=schema,\n                        api_version=self.api_version,\n                    )\n                    if res.choices[0].finish_reason == \"content_filter\":\n                        raise ValueError(f\"Response filtred by content filter\")\n                    else:\n                        dict_res = ast.literal_eval(res.choices[0].message.content)\n                        return schema(**dict_res)\n\n                else:\n                    client = instructor.from_litellm(completion, mode=instructor.Mode.JSON)\n                    res, raw_completion = client.chat.completions.create_with_completion(\n                        model=self.model_name,\n                        api_key=self.api_key.get_secret_value(),\n                        base_url=self.base_url,\n                        messages=messages,\n                        response_model=schema,\n                        api_version=self.api_version,\n                    )\n                    return res\n            else:\n                res = litellm.completion(\n                    model=self.model_name,\n                    api_key=self.api_key.get_secret_value(),\n                    base_url=self.base_url,\n                    messages=messages,\n                    api_version=self.api_version,\n                )\n\n                return res.choices[0].message.content\n        except Exception as e:\n            # todo handle cost if exception\n            logger.error(f\"Error in generating response from LLM: {e}\")\n            return None\n\n    def get_model_name(self, *args, **kwargs) -&gt; str:\n        return self.model_name\n</code></pre>"},{"location":"package/pages/0_chat/","title":"0 chat","text":""},{"location":"package/pages/1_embeddings/","title":"1 embeddings","text":""},{"location":"package/pages/2_azure_rag/","title":"2 azure rag","text":""},{"location":"package/pages/3_fastapi_azure_rag/","title":"3 fastapi azure rag","text":""}]}