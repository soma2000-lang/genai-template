# Learn more about building a configuration: https://promptfoo.dev/docs/configuration/guide
description: "My eval"

prompts:
  #  the {{query}} and {{context}} are the columns in the dataset (test_simple.csv)
  - |
    You are an internal corporate chatbot.
    Respond to this query: {{query}}
    - Here is some context that you can use to write your response: {{context}}


providers:
  - id: openai:chat:{{env.OPENAI_DEPLOYMENT_NAME}} # env variables are in .env
    label: '{{env.OPENAI_DEPLOYMENT_NAME}}'



defaultTest:
  assert:
# retrieval metrics: evaluating retrieved contexts against relevant contexts
    # Order unaware retrieval metrics
    - type: python
      value: file://../metrics/order_unaware/precision_at_k.py
      metric: PrecisionK
    - type: python
      value: file://../metrics/order_unaware/recall_at_k.py
      metric: RecallK
    - type: python
      value: file://../metrics/order_unaware/f1_at_k.py
      metric: F1K
    # Order aware retrieval metrics
    - type: python
      value: file://../metrics/order_aware/reciprocal_rank.py
      metric: Mean Reciprocal Rank

    # end-task: evaluating ground truth vs generated answer
    - type: equals
      value: '{{ground_truth}}'
    #    - type: python
    #      value: file://../metrics/ragas_metrics/ragas_answer_similarity.py
    #      metric: Ragas Answer Similarity
    #    - type: python
    #      value: file://../metrics/ragas_metrics/ragas_answer_correctness.py
    #      metric: Ragas Answer Correctness

    # evaluating answer vs retrieved context:
    #    - type: python
    #      value: file://../metrics/ragas_metrics/ragas_answer_relevancy.py
    #      metric: Ragas Answer Relevancy

    # retrieval metrics: evaluating retrieved contexts against ground truth
  #    - type: python
  #      value: file://../metrics/ragas_metrics/ragas_context_recall.py
  #      metric: Ragas Context Recall
  #    - type: python
  #      value: file://../metrics/ragas_metrics/ragas_context_precision.py
  #      metric: Ragas Context Precision
  #    - type: python
  #      value: file://../metrics/ragas_metrics/ragas_context_entity_recall.py
  #      metric: Ragas Context Entity Recall
  #    - type: python
  #      value: file://../metrics/ragas_metrics/ragas_context_utilization.py
  #      metric: Ragas Context Utilization



  ## latency needs cache to be off
  #   - type: latency
  #     threshold: 5000
tests:
  - file://../data/test_simple.csv
